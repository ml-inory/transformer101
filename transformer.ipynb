{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention is all you need\n",
    "[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img\\attention.png)  \n",
    "![scaled-dot-product-attention.png](img\\scaled-dot-product-attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask=None) -> torch.Tensor:\n",
    "        # q: (b, n, dk)\n",
    "        # k: (b, m, dk)\n",
    "        # v: (b, m, dk)\n",
    "\n",
    "        # (b, n, m)\n",
    "        x = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(q.size(-1))\n",
    "        if mask is not None:\n",
    "            x = x * mask\n",
    "        \n",
    "        # (b, n, dk)\n",
    "        return F.softmax(x, dim=-1).matmul(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么要除以根号dk？\n",
    "与Softmax函数两边的扁平区有关，两边的梯度接近于0，所以softmax进去的值不能特大或特小，然而\n",
    "Q.matmul(K.T)的方差可能比较大，容易造成梯度消失。\n",
    "假设Q和K的均值为0，方差为1，且Q和K独立，则Q.matmul(K.T)的方差 = sum to dk(var(Q * K)) = sum to dk(var(Q) * var(K)) = dk\n",
    "要让方差变为1:\n",
    "var(Q * K / sqrt(dk)) = 1 / dk * var(Q * K) = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal dot product softmax grad:\n",
      "grad of Q: 4.1509100157099965e-08\n",
      "grad of K: 6.693417731185036e-08\n",
      "Scaled dot product softmax grad:\n",
      "grad of Q: 1.873047040135134e-05\n",
      "grad of K: 3.744859714061022e-05\n"
     ]
    }
   ],
   "source": [
    "dk = 512\n",
    "Q = torch.rand([1024, dk], requires_grad=True)\n",
    "K = torch.rand([1024, dk], requires_grad=True)\n",
    "sm = torch.softmax(Q.matmul(K.T), dim=-1)\n",
    "sm[0, 0].backward()\n",
    "print(\"Normal dot product softmax grad:\")\n",
    "print(f\"grad of Q: {Q.grad.max()}\")\n",
    "print(f\"grad of K: {K.grad.max()}\")\n",
    "\n",
    "sm = torch.softmax(Q.matmul(K.T) / math.sqrt(dk), dim=-1)\n",
    "sm[0, 0].backward()\n",
    "print(\"Scaled dot product softmax grad:\")\n",
    "print(f\"grad of Q: {Q.grad.max()}\")\n",
    "print(f\"grad of K: {K.grad.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mha.png](img\\mha.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self, d_model, num_head):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_head = num_head\n",
    "        self.d_k = self.d_model // self.num_head\n",
    "        self.W_Q = [nn.Linear(d_model, self.d_k)] * self.num_head\n",
    "        self.W_K = [nn.Linear(d_model, self.d_k)] * self.num_head\n",
    "        self.W_V = [nn.Linear(d_model, self.d_k)] * self.num_head\n",
    "        self.attn = [ScaledDotProductAttention()] * self.num_head\n",
    "        self.linear_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # (b, n, d_k)\n",
    "        q_proj = [w_q(Q) for w_q in self.W_Q]\n",
    "        # (b, m, d_k)\n",
    "        k_proj = [w_k(K) for w_k in self.W_K]\n",
    "        v_proj = [w_v(V) for w_v in self.W_V]\n",
    "\n",
    "        attn_out = [attn(q, k, v, mask) for attn, q, k, v in zip(self.attn, q_proj, k_proj, v_proj)]\n",
    "\n",
    "        # (b, n, d_model)\n",
    "        return self.linear_out(torch.concat(attn_out, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3248,  0.0545,  0.0064,  ..., -0.2376, -0.0765,  0.0862],\n",
      "         [-0.3234,  0.0535,  0.0067,  ..., -0.2379, -0.0763,  0.0865],\n",
      "         [-0.3241,  0.0537,  0.0067,  ..., -0.2381, -0.0761,  0.0868],\n",
      "         ...,\n",
      "         [-0.3243,  0.0534,  0.0067,  ..., -0.2376, -0.0760,  0.0861],\n",
      "         [-0.3243,  0.0543,  0.0061,  ..., -0.2373, -0.0755,  0.0865],\n",
      "         [-0.3248,  0.0546,  0.0060,  ..., -0.2373, -0.0759,  0.0852]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch.Size([1, 16, 512])\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "num_head = 8\n",
    "\n",
    "batch_size = 1\n",
    "n = 16\n",
    "m = 32\n",
    "mha = MHA(d_model=d_model, num_head=num_head)\n",
    "Q = torch.rand([batch_size, n, d_model])\n",
    "K = torch.rand([batch_size, m, d_model])\n",
    "V = torch.rand([batch_size, m, d_model])\n",
    "mha_out = mha(Q, K, V)\n",
    "print(mha_out)\n",
    "print(mha_out.size())\n",
    "assert(mha_out.size() == torch.Size([batch_size, n, d_model]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![encode_block](img\\encode_block.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_ffn = 4 * d_model\n",
    "        self.ffn_hidden = nn.Linear(d_model, self.d_ffn)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.ffn_out = nn.Linear(self.d_ffn, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ffn_out(self.relu(self.ffn_hidden(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResMHA(nn.Module):\n",
    "    def __init__(self, d_model, num_head):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = MHA(d_model, num_head)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None, dropout=None):\n",
    "        mha_out = self.mha(Q=Q, K=K, V=V, mask=mask)\n",
    "        if dropout:\n",
    "            mha_out = dropout(mha_out)\n",
    "        x = self.norm(Q + mha_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResFFN(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ffn = FFN(d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, dropout=None):\n",
    "        ffn_out = self.ffn(x)\n",
    "        if dropout:\n",
    "            ffn_out = dropout(ffn_out)\n",
    "        x = self.norm(x + ffn_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodeBlock(nn.Module):\n",
    "    def __init__(self, d_model=512, num_head=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = ResMHA(d_model, num_head)\n",
    "        self.ffn = ResFFN(d_model)\n",
    "\n",
    "    def forward(self, x, dropout=None):\n",
    "        x = self.mha(Q=x, K=x, V=x, dropout=dropout)\n",
    "        x = self.ffn(x, dropout)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5173,  1.3850,  0.3295,  ...,  1.6806,  0.7333, -1.0889],\n",
      "         [-0.6384,  1.8470, -1.2366,  ..., -0.7849,  0.2638, -1.1329],\n",
      "         [ 0.0967, -0.2581, -0.5965,  ..., -0.3920,  1.5811, -1.4578],\n",
      "         ...,\n",
      "         [-0.0209,  1.1334, -0.2954,  ...,  1.5240,  0.9521, -2.5016],\n",
      "         [-0.8310,  0.9661,  0.4257,  ...,  0.1001, -0.1922, -0.9898],\n",
      "         [ 0.3043, -0.6394,  0.2250,  ...,  1.1292,  0.7230, -0.0559]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([1, 16, 512])\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "num_head = 8\n",
    "\n",
    "batch_size = 1\n",
    "n = 16\n",
    "m = 32\n",
    "Q = torch.rand([batch_size, n, d_model])\n",
    "\n",
    "encode_block = EncodeBlock(d_model, num_head)\n",
    "encode_out = encode_block(Q)\n",
    "print(encode_out)\n",
    "print(encode_out.size())\n",
    "assert(encode_out.size() == torch.Size([batch_size, n, d_model]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![decode_block](img\\decode_block.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.0000e+00, -1.0000e-09, -1.0000e-09, -1.0000e-09],\n",
      "         [ 1.0000e+00,  1.0000e+00, -1.0000e-09, -1.0000e-09],\n",
      "         [ 1.0000e+00,  1.0000e+00,  1.0000e+00, -1.0000e-09],\n",
      "         [ 1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00]]])\n"
     ]
    }
   ],
   "source": [
    "def sequence_mask(token_len):\n",
    "    mask = torch.ones([token_len, token_len])\n",
    "    tril_mask = torch.tril(mask) == 0\n",
    "    return mask.masked_fill(tril_mask, value=-1e-9).unsqueeze(0)\n",
    "\n",
    "mask = sequence_mask(4)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecodeBlock(nn.Module):\n",
    "    def __init__(self, d_model=512, num_head=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = ResMHA(d_model, num_head)\n",
    "        self.ffn = ResFFN(d_model)\n",
    "\n",
    "    def forward(self, x, encoder_out_kv, mask=None, dropout=None):\n",
    "        if mask is None:\n",
    "            mask = sequence_mask(x.size(-2))\n",
    "\n",
    "        q = self.mha(Q=x, K=x, V=x, mask=mask, dropout=dropout)\n",
    "        x = self.mha(Q=q, K=encoder_out_kv, V=encoder_out_kv, dropout=dropout)\n",
    "        x = self.ffn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.3745, -0.1947,  0.1542,  ...,  0.6360,  0.6947, -1.8688],\n",
      "         [ 1.6771, -0.3148, -0.9900,  ..., -0.6673,  1.1267, -1.0726],\n",
      "         [ 1.6385, -1.1807,  0.5586,  ...,  1.5278,  0.3707, -1.2273],\n",
      "         ...,\n",
      "         [ 0.1696, -0.5213, -0.2874,  ...,  1.8333,  0.9122, -1.6084],\n",
      "         [ 1.0831, -0.2956, -0.9377,  ...,  0.9470,  0.1393, -0.9638],\n",
      "         [ 1.8224, -1.7668, -0.7125,  ..., -0.6468,  0.7662, -0.8900]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([1, 16, 512])\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "num_head = 8\n",
    "\n",
    "batch_size = 1\n",
    "n = 16\n",
    "m = 32\n",
    "Q = torch.rand([batch_size, n, d_model])\n",
    "\n",
    "encode_block = EncodeBlock(d_model, num_head)\n",
    "encode_out = encode_block(Q)\n",
    "\n",
    "decode_block = DecodeBlock(d_model, num_head)\n",
    "decode_out = decode_block(Q, encode_out)\n",
    "print(decode_out)\n",
    "print(decode_out.size())\n",
    "assert(decode_out.size() == torch.Size([batch_size, n, d_model]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![overall](img\\overall_structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "![pe](img\\pe.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 8.4147e-01,  8.2186e-01,  8.0196e-01,  ...,  1.1140e-08,\n",
      "          1.0746e-08,  1.0366e-08],\n",
      "        [ 9.0930e-01,  9.3641e-01,  9.5814e-01,  ...,  2.2279e-08,\n",
      "          2.1492e-08,  2.0733e-08],\n",
      "        ...,\n",
      "        [-9.6612e-01,  7.4857e-01,  2.1454e-01,  ...,  6.7952e-07,\n",
      "          6.5551e-07,  6.3235e-07],\n",
      "        [-7.3918e-01, -1.1848e-01,  9.1145e-01,  ...,  6.9066e-07,\n",
      "          6.6626e-07,  6.4271e-07],\n",
      "        [ 1.6736e-01, -8.8357e-01,  8.7441e-01,  ...,  7.0180e-07,\n",
      "          6.7700e-07,  6.5308e-07]])\n"
     ]
    }
   ],
   "source": [
    "def position_encoding(token_num, d_model):\n",
    "    pe = torch.zeros([token_num, d_model])\n",
    "    pos = torch.arange(0, token_num).repeat(d_model, 1).T\n",
    "    power = 2.0 / d_model * torch.arange(0, d_model)\n",
    "    pe[:, ::2] = torch.sin(pos[:, ::2] / torch.pow(10000, power[::2]))\n",
    "    pe[:, 1::2] = torch.sin(pos[:, 1::2] / torch.pow(10000, power[1::2]))\n",
    "    return pe\n",
    "\n",
    "print(position_encoding(64, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layer=6, d_model=512, num_head=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layer = num_layer\n",
    "        self.d_model = d_model\n",
    "        self.num_head = num_head\n",
    "\n",
    "        self.encode_blocks = [EncodeBlock(d_model, num_head)] * num_layer\n",
    "\n",
    "    def forward(self, x, embedding: nn.Embedding, pe: torch.Tensor, dropout=None):\n",
    "        x = embedding(x) + pe\n",
    "        if dropout:\n",
    "            x = dropout(x)\n",
    "        outputs = []\n",
    "        for block in self.encode_blocks:\n",
    "            x = block(x, dropout=dropout)\n",
    "            outputs.append(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layer=6, d_model=512, num_head=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layer = num_layer\n",
    "        self.d_model = d_model\n",
    "        self.num_head = num_head\n",
    "\n",
    "        self.decode_blocks = [DecodeBlock(d_model, num_head)] * num_layer\n",
    "\n",
    "    def forward(self, x, embedding: nn.Embedding, pe: torch.Tensor, encoder_out_kv, mask=None, dropout=None):\n",
    "        x = embedding(x) * math.sqrt(self.d_model) + pe\n",
    "        if dropout:\n",
    "            x = dropout(x)\n",
    "        for block, kv in zip(self.decode_blocks, encoder_out_kv):\n",
    "            x = block(x, encoder_out_kv=kv, mask=mask, dropout=dropout)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, n_vocab, token_len, num_layer=6, d_model=512, num_head=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_vocab = n_vocab\n",
    "        self.num_layer = num_layer\n",
    "        self.d_model = d_model\n",
    "        self.num_head = num_head\n",
    "\n",
    "        self.embedding = nn.Embedding(n_vocab, d_model)\n",
    "        self.pe = position_encoding(token_len, d_model)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.encoder = Encoder(num_layer, d_model, num_head)\n",
    "        self.decoder = Decoder(num_layer, d_model, num_head)\n",
    "        self.vocab_linear = nn.Linear(d_model, n_vocab)\n",
    "        self.vocab_act = nn.ReLU()\n",
    "\n",
    "    def forward(self, enc_x, dec_x, mask=None):\n",
    "        enc_kv = self.encoder(enc_x, self.embedding, self.pe, dropout=self.dropout)\n",
    "        x = self.decoder(dec_x, self.embedding, self.pe, enc_kv, mask=mask, dropout=self.dropout)\n",
    "        x = self.vocab_act(self.vocab_linear(x))\n",
    "        return F.softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0007, 0.0010, 0.0007,  ..., 0.0011, 0.0007, 0.0007],\n",
      "         [0.0007, 0.0014, 0.0007,  ..., 0.0007, 0.0007, 0.0012],\n",
      "         [0.0007, 0.0016, 0.0007,  ..., 0.0010, 0.0007, 0.0010],\n",
      "         ...,\n",
      "         [0.0007, 0.0014, 0.0007,  ..., 0.0010, 0.0007, 0.0007],\n",
      "         [0.0007, 0.0015, 0.0007,  ..., 0.0008, 0.0007, 0.0020],\n",
      "         [0.0007, 0.0009, 0.0010,  ..., 0.0009, 0.0007, 0.0014]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([1, 32, 1024])\n"
     ]
    }
   ],
   "source": [
    "n_vocab = 1024\n",
    "token_len = 32\n",
    "model = Transformer(n_vocab, token_len)\n",
    "\n",
    "enc_x = torch.randint(low=0, high=n_vocab, size=[1, token_len])\n",
    "dec_x = torch.randint(low=0, high=n_vocab, size=[1, token_len])\n",
    "mask = sequence_mask(dec_x.size(-1))\n",
    "output = model(enc_x, dec_x, mask=mask)\n",
    "print(output)\n",
    "print(output.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
